# 优化集群成本

## 优化部署

你真的需要这么多副本吗？这个问题的答案似乎很明显，但是集群中的每个 Pod 都会占用一些资源，导致其他 Pod 无法使用这些资源。
为了确保某个 Pod 发生故障或在滚动升级期间，服务质量永远不会下降，我们常常忍不住想运行大量的副本。此外，副本越多，应用程序可以处理的流量就越大。

但是，我们应该理智地使用副本。集群只能运行有限数量的 Pod。我们应该将这些有限的 Pod 用到真正需要最大化可用性和性能的应用程序。

如果在升级过程中关闭某个部署几秒钟也没关系，那么就不需要大量的副本。很多应用程序和服务只需要一两个副本也完全可以正常运行。

重新审视为每个部署配置的副本数，然后问问自己：

- 该服务有哪些性能和可用性方面的业务要求？
- 我们是否可以用更少的副本满足这些要求？

如果应用在处理需求时很吃力，或者用户在升级部署期间遇到太多错误，则该应用需要更多副本。但在很多情况下，即使大幅缩减部署也不会引发用户能注意到的服务降级。

在满足性能和可用性要求的前提下，最小化部署使用的 Pod 数量。逐渐减少副本数量，直到刚好达到服务水平目标。

## 优化 Pod

为容器设置正确的资源请求和约束的非常重要。如果资源请求设置得太小，那么很快你就会知道，因为 Pod 会出现问题。但是，如果资源请求设置得太大，那么你要等收到第一个月的云账单时才会发现。

应该定期审查各种工作负载的资源请求和约束，并与实际使用的资源进行比较。

大多数托管的 Kubernetes 服务都提供某种形式的仪表板，显示一段时间内容器使用 CPU 和内存的情况。可以使用 Prometheus 和 Grafana 构建自己的仪表板和统计信息。

设置最理想的资源请求值和约束值是一门艺术，每种工作负载的理想值都不同。有些容器可能在大多数时候都是空闲状态，偶尔在处理某个请求时才会出现资源使用的高峰。而有些容器始终忙忙碌碌，而且使用的内存越来越多，直到达到约束值。

通常情况下，容器的资源约束应该略高于正常操作中使用的最大资源量。如在一段时间内容器的内存使用量从未超过 500 MiB ，则可以设置其内存限制为 600 MiB。

可以考虑使用 Pod 垂直自动伸缩器的 Kubernetes 插件，它可以帮助你找出资源请求的理想值。它会监视指定的部署，并根据实际使用情况自动调整 Pod 的资源请求。它提供演习模式，而且在演习模式下只提供建议，并不会真正执行操作（修改正在运行的 Pod）。

## 优化节点

Kubernetes 支持各种大小的节点，但其中某些节点的性能会更好一些。为了让付出的资金能换来更理想的集群容量，需要在真实的需求条件下，运行真实的工作负载，并观察节点的实际性能。这种方式可以帮助你确定性价比最高的实例类型。

每个节点上都必须有一个操作系统，而这个操作系统会占用磁盘、内存和 CPU 资源。Kubernetes 系统组件和容器运行时也是如此。节点越小，这部分开销所占的总资源比例就越大。

因此，节点越大，工作负载所占的资源比例就越大，性价比就越高。而代价是失去某个节点对集群的可用容量产生的影响也就越大。

小型节点更有可能出现被搁浅资源的现象。被搁浅资源指的是，虽然存在未使用的内存空间和 CPU 时间，但对任何现有的 Pod 来说都太小了，导致无法分配。

一条很好的经验法则是，节点应该能够运行至少 5 个典型的 Pod，并将被搁浅资源的比例保持在 10% 以下。如果节点可以运行 10 个或更多 Pod，则被搁浅资源应低于 5%。

Kubernetes 中默认的约束是每个节点 110 个 Pod。尽管你可以通过调整 `kubelet` 的`--max-pods`设置来提高这个约束，但在某些托管服务中无法这样做，而且最好还是保留 Kubernetes 的默认值，除非你有充分的更改理由。

每个节点上的 Pod 数量约束意味着云提供商支持的最大实例无法得到完全利用。相反，你应该考虑运行大量较小的节点，以获得更好的利用率。例如，不要运行 6 个拥有 8 个 vCPU 的节点，而是改为运行 12 个拥有 4 个 vCPU 的节点。

如果较大的节点利用率较低，则表明集群的容量过大，因此可以删除某些节点，或者缩减节点的大小，从而降低总费用。

> 可以通过云提供商的仪表盘或者`kubectl top nodes`，查看每个节点资源利用率百分比，CPU 的使用百分比越大，利用率就越好。如果集群中较大的节点利用率较好，可以考虑删除一些小节点，并替换成更大的节点。节点越大，操作系统的开销消耗的资源就会越小

## 优化存储

磁盘存储的云成本常常被忽略。云提供商为不同大小的实例提供不同大小的磁盘空间，各种大型存储的价格也各异。

虽然使用 Kubernetes 资源请求和约束可以实现很高的 CPU 和内存利用率，但存储却无法做到这一点，而且许多集群节点配置的磁盘空间都远远超过了需要。

除了许多节点拥有超出需求的存储空间之外，存储类型也可能是一个因素。大多数云提供商都会根据每秒 I/O 操作数（I/O operations per second， IOPS）或占用的带宽提供不同类别的存储。

例如，使用持久性磁盘卷的数据库通常需要很高的 IOPS，才能实现快速、高吞吐量的存储访问。这类存储很昂贵。为了节省云成本，你可以为不需要那么多带宽的工作负载配置低 IOPS 存储。另一方面，如果你的应用程序在等待存储 I/O 上花费了大量时间而导致性能很差，则应该提供更多 IOPS 来解决这个问题。

通常，云或 Kubernetes 提供商的控制台能够显示节点上实际使用了多少 IOPS，而且你可以通过这些数字来决定削减何处的成本。

> 不要使用存储空间超过实际需要的实例类型。根据实际使用的吞吐量和空间，尽可能使用最小，IOPS 最低的磁盘卷。

## 清理未使用的资源

随着 Kubernetes 集群的增长，你会发现许多未使用或丢失的资源在黑暗的角落里徘徊。长此以往，如果这些丢失的资源没有得到清理，它们将占据总成本的很大一部分。

在最高层面，你可能会发现一些不属于任何集群的云实例，因为你很容易忘记关闭某台不再使用的机器。

还有一些类型的云资源即使不使用也要花钱，例如负载均衡器、公共 IP 和磁盘卷。你应该定期检查每种资源的使用情况，找出并删除未使用的实例。

同理， Kubernetes 集群中的某些部署或 Pod 可能并没有被任何服务实际引用，因此它们无法接收到流量。可以检查 CPU 和内存的利用率，找出长时间低利用率的资源并考虑清理它。

那些没有运行的容器镜像也会占据节点上的磁盘空间。幸运的是，当节点上的磁盘空间不足时，Kubernetes 会自动清理未使用的镜像。

尽可能为每一个资源加上一个标签注释（元数据）来说明创建者是谁，这样可以在清理资源或准备优化集群时能够找到对应的人，了解资源使用的情况。不过也别对此要求太过苛刻，要明白开发人员并无恶意。

Kubernetes 作业只运行一次就完成了，并且不会重新启动。但是，Job 对象仍然会留在 Kubernetes 的数据库中，如果存在大量已完成的 Job，就有可能影响 API 的性能。可以考虑使用 kube-job-cleaner 工具清理已经完成的作业。

## 备用资源

无论何时，集群都应该有足够的备用容量来应对某个工作节点发生故障。如果想检查备用容量是否足够，你可以把最大的节点排干。在节点上所有的 Pod 都被驱逐之后，检查所有应用程序是否仍能够工作，且副本数能够达到配置的要求。如果不满足，则需要向集群添加更多容量。

如果在节点发生故障时，没有足够的容量重新安排工作负载，则最好的结果就是服务会降级，而在最糟糕的情况下服务将无法使用。

有些云提供商会根据计算机的生命周期提供不同的实例类。预留实例可在价格和灵活性之间取得平衡。

如果你很清楚可预见未来的需求，则预留实例是一个不错的选择。但是，没有使用的预留也不予退款，你必须提前付清整个预留期的费用。因此，只有在需求不太可能发生显著变化的期间内，才应该选择预留实例。如果你能够提前做出一年或两年的计划，那么使用预留实例可以省一大笔钱。

> 预留实例（Reserved Instances，简称 RI）是一种预付费模式，它并非真正的物理实例，而是一种对用户使用的按量计费物理实例所应用到的账单折扣，本质上仍然是按量计费模式。（摘自某云服务产品介绍页）

## 抢占式（Spot）实例

AWS 称这种实例为 Spot 实例，Google 称之为抢占式虚拟机，它们不提供可用性保证，而且生命周期通常都是有限的。因此，它们是价格与可用性之间的折中。

Spot 实例很便宜，但随时可能被暂停或继续，并且可能被完全终止。幸运的是，即便丢失某个集群节点，Kubernetes 仍可提供高可用的服务。

使用抢占式节点是减少 Kubernetes 集群成本的一种非常有效的方法。尽管你可能需要多运行几个节点，以确保工作负载能够在抢占发生时存活下来，但事实证明，抢占式节点可以将每个节点的成本降到一半。

但是请记住，你应该确保足够的不可抢占节点来处理集群最低限度的工作负载。风险不能超过你的承受范围。如果你有很多抢占式节点，那么最好使用集群自动伸缩来确保尽快替换所有被抢占的节点

从理论上讲，有可能所有可抢占节点会同时消失。因此，尽管能够节省或本，但最好还是将抢占式节点限制在集群的三分之二以内。让某些节点使用抢占式节点或 Spot 实例可以降低成本，但是不要
让损失超过你能够承受的范围。务必保留一些非抢占式节点。

## 节点亲和性

你可以使用 Kubernetes 的节点亲和性来保证无法承受失败的 Pod 不会被调度到抢占式节点上。

设置在`.spec.affinity.nodeAffinity`中

- `requiredDuringSchedulingIgnoredDuringExecution`：调度器只有在规则被满足的时候才能执行调度。此功能类似于 nodeSelector，但其语法表达能力更强。凡是拥有该亲和性的 Pod 永远不会被调度到与选择器表达式不匹配的节点上（硬亲和性）
- `preferredDuringSchedulingIgnoredDuringExecution`：调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。有些不太重要的 Pod （允许偶尔出现故障）可以设置其优先被调度到抢占式节点上（软亲和性）

## 保持工作负载

Kubernetes 调度器会确保工作负载均匀地分散到尽可能多的节点中，并尝试将 Pod 副本放置在不同的节点上以实现高可用性。一般来讲，调度器可以很好地完成这些工作，但是你需要注意一些极端情况。

当节点出现故障时，Kubernetes 调度器会重新创建副本到其它节点上，有可能会出现本身均匀分布的 Pod 都集中到了一两个节点上，即使节点故障处理后重新恢复了，Kubernetes 调度器也不会重新去调度 Pod（因为调度器认为当前的 Pod 已经满足资源要求）。

解决这个问题的方法之一是使用一种名叫解调度器的工具。你可以频繁运行这个工具，就像 Kubernetes 作业一样，它会尽力重新平衡集群，方法是找到需要移动的 Pod 并干掉它们。

解调度器具有可供配置的各种策略和方针。例如，有一种策略会寻找未充分利用的节点，并杀死其他节点上的 Pod，强制将它们重新调度到这个空闲的节点上。还有一种策略是寻找重复的 Pod，即找到在同一个节点上运行的同一个 Pod 的两个或多个副本，并驱逐它们。

Last Modified 2023-06-07

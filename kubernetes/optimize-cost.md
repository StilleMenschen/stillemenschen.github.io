# 优化集群成本

## 优化部署

你真的需要这么多副本吗？这个问题的答案似乎很明显，但是集群中的每个 Pod 都会占用一些资源，导致其他 Pod 无法使用这些资源。
为了确保某个 Pod 发生故障或在滚动升级期间，服务质量永远不会下降，我们常常忍不住想运行大量的副本。此外，副本越多，应用程序可以处理的流量就越大。

但是，我们应该理智地使用副本。集群只能运行有限数量的 Pod。我们应该将这些有限的 Pod 用到真正需要最大化可用性和性能的应用程序。

如果在升级过程中关闭某个部署几秒钟也没关系，那么就不需要大量的副本。很多应用程序和服务只需要一两个副本也完全可以正常运行。

重新审视为每个部署配置的副本数，然后问问自己：

- 该服务有哪些性能和可用性方面的业务要求？
- 我们是否可以用更少的副本满足这些要求？

如果应用在处理需求时很吃力，或者用户在升级部署期间遇到太多错误，则该应用需要更多副本。但在很多情况下，即使大幅缩减部署也不会引发用户能注意到的服务降级。

在满足性能和可用性要求的前提下，最小化部署使用的 Pod 数量。逐渐减少副本数量，直到刚好达到服务水平目标。

## 优化 Pod

为容器设置正确的资源请求和约束的非常重要。如果资源请求设置得太小，那么很快你就会知道，因为 Pod 会出现问题。但是，如果资源请求设置得太大，那么你要等收到第一个月的云账单时才会发现。

应该定期审查各种工作负载的资源请求和约束，并与实际使用的资源进行比较。

大多数托管的 Kubernetes 服务都提供某种形式的仪表板，显示一段时间内容器使用 CPU 和内存的情况。可以使用 Prometheus 和 Grafana 构建自己的仪表板和统计信息。

设置最理想的资源请求值和约束值是一门艺术，每种工作负载的理想值都不同。有些容器可能在大多数时候都是空闲状态，偶尔在处理某个请求时才会出现资源使用的高峰。而有些容器始终忙忙碌碌，而且使用的内存越来越多，直到达到约束值。

通常情况下，容器的资源约束应该略高于正常操作中使用的最大资源量。如在一段时间内容器的内存使用量从未超过 500 MiB ，则可以设置其内存限制为 600 MiB。

可以考虑使用 Pod 垂直自动伸缩器的 Kubernetes 插件，它可以帮助你找出资源请求的理想值。它会监视指定的部署，并根据实际使用情况自动调整 Pod 的资源请求。它提供演习模式，而且在演习模式下只提供建议，并不会真正执行操作（修改正在运行的 Pod）。

## 优化节点

Kubernetes 支持各种大小的节点，但其中某些节点的性能会更好一些。为了让付出的资金能换来更理想的集群容量，需要在真实的需求条件下，运行真实的工作负载，并观察节点的实际性能。这种方式可以帮助你确定性价比最高的实例类型。

每个节点上都必须有一个操作系统，而这个操作系统会占用磁盘、内存和 CPU 资源。Kubernetes 系统组件和容器运行时也是如此。节点越小，这部分开销所占的总资源比例就越大。

因此，节点越大，工作负载所占的资源比例就越大，性价比就越高。而代价是失去某个节点对集群的可用容量产生的影响也就越大。

小型节点更有可能出现被搁浅资源的现象。被搁浅资源指的是，虽然存在未使用的内存空间和 CPU 时间，但对任何现有的 Pod 来说都太小了，导致无法分配。

一条很好的经验法则是，节点应该能够运行至少 5 个典型的 Pod，并将被搁浅资源的比例保持在 10% 以下。如果节点可以运行 10 个或更多 Pod，则被搁浅资源应低于 5%。

Kubernetes 中默认的约束是每个节点 110 个 Pod。尽管你可以通过调整 `kubelet` 的`--max-pods`设置来提高这个约束，但在某些托管服务中无法这样做，而且最好还是保留 Kubernetes 的默认值，除非你有充分的更改理由。

每个节点上的 Pod 数量约束意味着云提供商支持的最大实例无法得到完全利用。相反，你应该考虑运行大量较小的节点，以获得更好的利用率。例如，不要运行 6 个拥有 8 个 vCPU 的节点，而是改为运行 12 个拥有 4 个 vCPU 的节点。

如果较大的节点利用率较低，则表明集群的容量过大，因此可以删除某些节点，或者缩减节点的大小，从而降低总费用。

> 可以通过云提供商的仪表盘或者`kubectl top nodes`，查看每个节点资源利用率百分比，CPU 的使用百分比越大，利用率就越好。如果集群中较大的节点利用率较好，可以考虑删除一些小节点，并替换成更大的节点。节点越大，操作系统的开销消耗的资源就会越小

## 优化存储

磁盘存储的云成本常常被忽略。云提供商为不同大小的实例提供不同大小的磁盘空间，各种大型存储的价格也各异。

虽然使用 Kubernetes 资源请求和约束可以实现很高的 CPU 和内存利用率，但存储却无法做到这一点，而且许多集群节点配置的磁盘空间都远远超过了需要。

除了许多节点拥有超出需求的存储空间之外，存储类型也可能是一个因素。大多数云提供商都会根据每秒 I/O 操作数（I/O operations per second， IOPS）或占用的带宽提供不同类别的存储。

例如，使用持久性磁盘卷的数据库通常需要很高的 IOPS，才能实现快速、高吞吐量的存储访问。这类存储很昂贵。为了节省云成本，你可以为不需要那么多带宽的工作负载配置低 IOPS 存储。另一方面，如果你的应用程序在等待存储 I/O 上花费了大量时间而导致性能很差，则应该提供更多 IOPS 来解决这个问题。

通常，云或 Kubernetes 提供商的控制台能够显示节点上实际使用了多少 IOPS，而且你可以通过这些数字来决定削减何处的成本。

> 不要使用存储空间超过实际需要的实例类型。根据实际使用的吞吐量和空间，尽可能使用最小，IOPS 最低的磁盘卷。

## 清理未使用的资源

随着 Kubernetes 集群的增长，你会发现许多未使用或丢失的资源在黑暗的角落里徘徊。长此以往，如果这些丢失的资源没有得到清理，它们将占据总成本的很大一部分。

在最高层面，你可能会发现一些不属于任何集群的云实例，因为你很容易忘记关闭某台不再使用的机器。

还有一些类型的云资源即使不使用也要花钱，例如负载均衡器、公共 IP 和磁盘卷。你应该定期检查每种资源的使用情况，找出并删除未使用的实例。

同理， Kubernetes 集群中的某些部署或 Pod 可能并没有被任何服务实际引用，因此它们无法接收到流量。可以检查 CPU 和内存的利用率，找出长时间低利用率的资源并考虑清理它。

那些没有运行的容器镜像也会占据节点上的磁盘空间。幸运的是，当节点上的磁盘空间不足时，Kubernetes 会自动清理未使用的镜像。

尽可能为每一个资源加上一个标签注释（元数据）来说明创建者是谁，这样可以在清理资源或准备优化集群时能够找到对应的人，了解资源使用的情况。不过也别对此要求太过苛刻，要明白开发人员并无恶意。

Kubernetes 作业只运行一次就完成了，并且不会重新启动。但是，Job 对象仍然会留在 Kubernetes 的数据库中，如果存在大量已完成的 Job，就有可能影响 API 的性能。可以考虑使用 kube-job-cleaner 工具清理已经完成的作业。

Last Modified 2023-06-06
